{



 "cells": [



  {



   "cell_type": "code",



   "execution_count": 1,



   "metadata": {},



   "outputs": [



    {



     "name": "stderr",



     "output_type": "stream",



     "text": [



      "2023-11-07 23:08:28.411832: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",



      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",



      "2023-11-07 23:08:28.579046: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",



      "2023-11-07 23:08:30.119332: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",



      "2023-11-07 23:08:30.119526: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",



      "2023-11-07 23:08:30.119549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"



     ]



    }



   ],



   "source": [



    "import json\n",



    "import pandas as pd\n",



    "import warnings\n",



    "import tqdm\n",



    "import os\n",



    "import string\n",



    "import re\n",



    "import csv\n",



    "warnings.filterwarnings('ignore')\n",



    "from rouge import Rouge\n",



    "rouge = Rouge()\n",



    "import evaluate"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 2,



   "metadata": {},



   "outputs": [],



   "source": [



    "bertscore = evaluate.load('bertscore')\n",



    "em = evaluate.load('exact_match')"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 3,



   "metadata": {},



   "outputs": [],



   "source": [



    "preds = []\n",



    "refs = []"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 4,



   "metadata": {},



   "outputs": [],



   "source": [



    "from collections import defaultdict"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 5,



   "metadata": {},



   "outputs": [],



   "source": [



    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 6,



   "metadata": {},



   "outputs": [],



   "source": [



    "langScoreRefs = {'hi':[],'en':[],'bn':[],'ta':[],'or':[],'pa':[]}\n",



    "langScorePreds = {'hi':[],'en':[],'bn':[],'ta':[],'or':[],'pa':[]}\n",



    "langScore = {'hi':[],'en':[],'bn':[],'ta':[],'or':[],'pa':[]}\n",



    "fin = open('final_article.csv', mode ='r')\n",



    "csvData = csv.reader(fin)\n",



    "for i,lineData in enumerate(csvData):\n",



    "    if i == 0:\n",



    "        continue\n",



    "    lang = lineData[1][:2]\n",



    "    x = lineData[3].split(' ')\n",



    "    y = lineData[2].split(' ')\n",



    "    \n",



    "    x = [i for i in x if i!= '']\n",



    "    y = [i for i in y if i!= '']\n",



    "    \n",



    "    if len(x) > len(y):\n",



    "        y = y + ['<pad>']*(len(x) - len(y))\n",



    "    elif len(y) > len(x):\n",



    "        x = x + ['<pad>']*(len(y) - len(x))\n",



    "        \n",



    "#     results = em.compute(predictions=x, references=y)\n",



    "#     langScore[lang].append(round(results['exact_match'], 2))\n",



    "#     results = bertscore.compute(predictions=[lineData[3]], references=[lineData[2]], model_type=\"bert-base-multilingual-cased\")\n",



    "#     langScore[lang].append(results['f1'][0])\n",



    "    \n",



    "    langScorePreds[lang].extend(x)\n",



    "    langScoreRefs[lang].extend(y)\n",



    "fin.close()"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 8,



   "metadata": {},



   "outputs": [],



   "source": [



    "for key in langScore.keys():\n",



    "    if len(langScore[key]) != 0:\n",



    "        print(f\"For language: {key}, average Exact Match score is: {sum(langScore[key])/len(langScore[key])}\")"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 9,



   "metadata": {},



   "outputs": [



    {



     "name": "stdout",



     "output_type": "stream",



     "text": [



      "hi {'acc': 0.630667578035999, 'p': 0.6576921870681213, 'r': 0.630667578035999, 'f1': 0.640187223670143}\n",



      "en {'acc': 0.46185286103542234, 'p': 0.5250970933780474, 'r': 0.46185286103542234, 'f1': 0.4759085933734836}\n",



      "bn {'acc': 0.5898498187467633, 'p': 0.5013551158220036, 'r': 0.5898498187467633, 'f1': 0.5278847985683925}\n",



      "ta {'acc': 0.6036179835062516, 'p': 0.7498223206932829, 'r': 0.6036179835062516, 'f1': 0.65691687773671}\n",



      "or {'acc': 0.43284671532846714, 'p': 0.6185370812282622, 'r': 0.43284671532846714, 'f1': 0.49426936096750496}\n",



      "pa {'acc': 0.3891242937853107, 'p': 0.4442298121478621, 'r': 0.3891242937853107, 'f1': 0.40775863227103276}\n"



     ]



    }



   ],



   "source": [



    "scores = defaultdict(float)\n",



    "for ln in langScoreRefs.keys():\n",



    "    y_true = langScoreRefs[ln]\n",



    "    y_pred = langScorePreds[ln]\n",



    "    metric_dict = {'acc': accuracy_score(y_true, y_pred),\n",



    "                    'p': precision_score(y_true, y_pred,average='weighted'),\n",



    "                    'r': recall_score(y_true, y_pred,average='weighted'),\n",



    "                    'f1': f1_score(y_true, y_pred,average='weighted')}\n",



    "    \n",



    "    print(ln, metric_dict)\n",



    "    \n",



    "#     score = rouge.get_scores(all_preds, all_refs, avg=True)\n",



    "#     scores[ln] = score['rouge-l']['f']"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 10,



   "metadata": {},



   "outputs": [



    {



     "data": {



      "text/plain": [



       "defaultdict(float, {})"



      ]



     },



     "execution_count": 10,



     "metadata": {},



     "output_type": "execute_result"



    }



   ],



   "source": [



    "scores"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 6,



   "metadata": {},



   "outputs": [



    {



     "name": "stderr",



     "output_type": "stream",



     "text": [



      "100%|██████████| 6/6 [00:45<00:00,  7.60s/it]\n"



     ]



    }



   ],



   "source": [



    "langScore = {'hi':[],'en':[],'bn':[],'ta':[],'or':[],'pa':[]}\n",



    "for lang in tqdm.tqdm(langScore.keys()):\n",



    "    gptfin = open(f'clfe_scrapes/merged_outputs/{lang}_gpt.csv', mode ='r')\n",



    "    reffin = open(f'/scratch/shivansh.s/intersection/{lang}_test.json', mode='r')\n",



    "    gptData = csv.reader(gptfin)\n",



    "    refData = []\n",



    "    for line in reffin:\n",



    "        refData.append(json.loads(line))\n",



    "    for i,lineData in enumerate(gptData):\n",



    "        if i == 0:\n",



    "            continue\n",



    "        for reflineData in refData:\n",



    "            if lineData[4] == reflineData[\"qid\"]:\n",



    "                refFact = \"\"\n",



    "                for fact in reflineData[\"xalign_facts\"]:\n",



    "                    refFact += fact[0].lower() + \" \" + fact[1].lower() + \" \"\n",



    "                    for punc in string.punctuation:\n",



    "                        refFact = refFact.replace(punc,' ')\n",



    "                gptFact = lineData[2]\n",



    "                gptFact = re.sub(r'<\\|im_end\\|>',r'', gptFact.lower())\n",



    "                for punc in string.punctuation:\n",



    "                    gptFact = gptFact.replace(punc,' ')\n",



    "                gptFact = re.sub(' +', ' ', gptFact.strip())\n",



    "                refFact = re.sub(' +', ' ', refFact.strip())\n",



    "                # score = rouge.get_scores(gptFact, refFact, avg=True)\n",



    "                # langScore[lang].append(score['rouge-l']['f'])\n",



    "\n",



    "                score = bertscore.compute(predictions=[gptFact], references=[refFact], model_type=\"bert-base-multilingual-cased\")\n",



    "                langScore[lang].append(score['f1'][0])    \n",



    "    gptfin.close()\n",



    "    reffin.close()"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 40,



   "metadata": {},



   "outputs": [



    {



     "name": "stdout",



     "output_type": "stream",



     "text": [



      "Rouge-L score for lang: hi is 0.5963631718622646\n",



      "Rouge-L score for lang: en is 0.6563214898029931\n",



      "Rouge-L score for lang: bn is 0.9022970499499996\n",



      "Rouge-L score for lang: ta is 0.7659665247600201\n",



      "Rouge-L score for lang: or is 0.599688542430034\n",



      "Rouge-L score for lang: pa is 0.6006571739311406\n"



     ]



    }



   ],



   "source": [



    "for key in langScore.keys():\n",



    "    print(f\"Rouge-L score for lang: {key} is {sum(langScore[key])/len(langScore[key])}\")"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": 7,



   "metadata": {},



   "outputs": [



    {



     "name": "stdout",



     "output_type": "stream",



     "text": [



      "Bert score for lang: hi is 0.832969351051506\n",



      "Bert score for lang: en is 0.868092382395709\n",



      "Bert score for lang: bn is 0.9537793087287688\n",



      "Bert score for lang: ta is 0.9018094785073224\n",



      "Bert score for lang: or is 0.8222764887436917\n",



      "Bert score for lang: pa is 0.84666516322356\n"



     ]



    }



   ],



   "source": [



    "for key in langScore.keys():\n",



    "    print(f\"Bert score for lang: {key} is {sum(langScore[key])/len(langScore[key])}\")"



   ]



  },



  {



   "cell_type": "code",



   "execution_count": null,



   "metadata": {},



   "outputs": [],



   "source": []



  }



 ],



 "metadata": {



  "kernelspec": {



   "display_name": "Python 3 (ipykernel)",



   "language": "python",



   "name": "python3"



  },



  "language_info": {



   "codemirror_mode": {



    "name": "ipython",



    "version": 3



   },



   "file_extension": ".py",



   "mimetype": "text/x-python",



   "name": "python",



   "nbconvert_exporter": "python",



   "pygments_lexer": "ipython3",



   "version": "3.9.7"



  }



 },



 "nbformat": 4,



 "nbformat_minor": 2



}



